{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# for basic mathematical operations\nimport numpy as np \nimport pandas as pd \n\n# for data visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# for defining a path for the dataset\nimport os\nprint(os.listdir(\"../input\"))\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# reading the dataset\n\ntrain = pd.read_csv('../input/X_train.csv')\nsub = pd.read_csv('../input/sample_submission.csv')\ntest = pd.read_csv('../input/X_test.csv')\ny_train = pd.read_csv('../input/y_train.csv')\n\n# getting the shapes of the datasets\nprint(\"Shape of train :\", train.shape)\nprint(\"Shape of test :\", test.shape)\nprint(\"Shape of y_train :\", y_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking the x_train\n\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking the test head\n\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# describing the train dataset\n\ntrain.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking if x_train and x_test contains any NULL values in the dataset\n\nprint(\"Null Values in the training set :\")\nprint(train.isnull().sum())\nprint(\"NULL Values in the testing set :\")\nprint(test.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking the head of the y_train set\n\ny_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking the unique group_id \n\nprint(\"Unique Elements present in the Group-id :\", y_train['group_id'].nunique())\nprint(\"Unique Elements present in the Series-id :\",y_train['series_id'].nunique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking the different types of surface\n\ny_train['surface'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plotting a pie chart\n\nsize = [779, 732, 607, 514, 363, 308, 297, 189, 21]\ncolors = ['pink', 'yellow', 'lightgreen', 'lightblue', 'purple', 'violet', 'crimson', 'darkred',\n          'blue']\nlabels = 'concrete', 'soft_pvc', 'wood','tiled','fine_concrete','hard_tiles_large_space','soft_tiles','carpet','hard_tiles'\n\nmy_circle = plt.Circle((0, 0), 0.7, color = 'white')\n\nplt.rcParams['figure.figsize'] = (10, 10)\nplt.pie(size, colors = colors, labels = labels, autopct = '%.2f%%')\nplt.title('Donut Chart to Represent different Surface Types', fontsize = 30)\nplt.axis('off')\np = plt.gcf()\np.gca().add_artist(my_circle)\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualizing the Group-ids\n\nplt.rcParams['figure.figsize'] = (20, 8)\nsns.countplot(y_train['group_id'], palette = 'pastel')\nplt.title('Frequency of Different Group-IDs', fontsize = 30)\nplt.xlabel('Group IDs', fontsize = 15)\nplt.ylabel('Count', fontsize = 15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting a heatmap for surface against groupid\n\ntmp = pd.DataFrame(y_train.groupby(['group_id', 'surface'])['series_id'].count().reset_index())\n\nx = tmp.pivot(index = 'surface', columns = 'group_id', values = 'series_id')\np = sns.heatmap(x, linewidths=.1, linecolor = 'black', annot = True, cmap = \"Reds\")\np.set_title('Number of surface category per group_id', size = 30)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"series_1 = train.head(128)\nseries_1.head()\n\n# plotting Time-Series Graph for all the attributes present in the training set\n\nplt.rcParams['figure.figsize'] = (15, 15)\nfor i, col in enumerate(series_1.columns[3:]):\n    plt.subplot(3, 4, i + 1)\n    plt.plot(series_1[col])\n    plt.title(col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"series_2 = train.head(128)\nseries_2.head()\n\n# plotting Time-Series Graph for all the attributes present in the training set\n\nplt.rcParams['figure.figsize'] = (15, 15)\nfor i, col in enumerate(series_2.columns[3:]):\n    plt.subplot(3, 4, i + 1)\n    plt.plot(series_2[col])\n    plt.title(col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking the distribution of all the attributes in the training and testing data\n\nplt.rcParams['figure.figsize'] = (15, 15)\nfor i, col in enumerate(train.columns[3:]):\n    plt.subplot(3, 4, i + 1)\n    plt.hist(train[col], color='blue', bins=100)\n    plt.hist(test[col], color='green', bins=100)\n    plt.title(col)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# heatmap of correlation for the attributes of the training set\n\nf,ax = plt.subplots(figsize=(12,6))\nm = train.iloc[:,3:].corr()\nsns.heatmap(m, annot=True, linecolor='darkblue', linewidths=.1, cmap = 'Reds', fmt= '.1f',ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_feature_distribution(df1, df2, label1, label2, features):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(2,5,figsize=(16,8))\n\n    for feature in features:\n        i += 1\n        plt.subplot(2,5,i)\n        sns.distplot(df1[feature], hist=False, label=label1)\n        sns.distplot(df2[feature], hist=False, label=label2)\n        plt.xlabel(feature, fontsize=9)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='x', which='major', labelsize=8)\n        plt.tick_params(axis='y', which='major', labelsize=8)\n    plt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_feature_class_distribution(classes,tt, features):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(5,2,figsize=(16,24))\n\n    for feature in features:\n        i += 1\n        plt.subplot(5,2,i)\n        for clas in classes:\n            ttc = tt[tt['surface']==clas]\n            sns.distplot(ttc[feature], hist=False,label=clas)\n        plt.xlabel(feature, fontsize=9)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='x', which='major', labelsize=8)\n        plt.tick_params(axis='y', which='major', labelsize=8)\n    plt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://stackoverflow.com/questions/53033620/how-to-convert-euler-angles-to-quaternions-and-get-the-same-euler-angles-back-fr?rq=1\ndef quaternion_to_euler(x, y, z, w):\n    import math\n    t0 = +2.0 * (w * x + y * z)\n    t1 = +1.0 - 2.0 * (x * x + y * y)\n    X = math.atan2(t0, t1)\n\n    t2 = +2.0 * (w * y - z * x)\n    t2 = +1.0 if t2 > +1.0 else t2\n    t2 = -1.0 if t2 < -1.0 else t2\n    Y = math.asin(t2)\n\n    t3 = +2.0 * (w * z + x * y)\n    t4 = +1.0 - 2.0 * (y * y + z * z)\n    Z = math.atan2(t3, t4)\n\n    return X, Y, Z","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def perform_euler_factors_calculation(df):\n    df['total_angular_velocity'] = np.sqrt(np.square(df['angular_velocity_X']) + np.square(df['angular_velocity_Y']) + np.square(df['angular_velocity_Z']))\n    df['total_linear_acceleration'] = np.sqrt(np.square(df['linear_acceleration_X']) + np.square(df['linear_acceleration_Y']) + np.square(df['linear_acceleration_Z']))\n    df['total_xyz'] = np.sqrt(np.square(df['orientation_X']) + np.square(df['orientation_Y']) +\n                              np.square(df['orientation_Z']))\n    df['acc_vs_vel'] = df['total_linear_acceleration'] / df['total_angular_velocity']\n    \n    x, y, z, w = df['orientation_X'].tolist(), df['orientation_Y'].tolist(), df['orientation_Z'].tolist(), df['orientation_W'].tolist()\n    nx, ny, nz = [], [], []\n    for i in range(len(x)):\n        xx, yy, zz = quaternion_to_euler(x[i], y[i], z[i], w[i])\n        nx.append(xx)\n        ny.append(yy)\n        nz.append(zz)\n    \n    df['euler_x'] = nx\n    df['euler_y'] = ny\n    df['euler_z'] = nz\n    \n    df['total_angle'] = np.sqrt(np.square(df['euler_x']) + np.square(df['euler_y']) + np.square(df['euler_z']))\n    df['angle_vs_acc'] = df['total_angle'] / df['total_linear_acceleration']\n    df['angle_vs_vel'] = df['total_angle'] / df['total_angular_velocity']\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def perform_feature_engineering(df):\n    df_out = pd.DataFrame()\n    \n    def mean_change_of_abs_change(x):\n        return np.mean(np.diff(np.abs(np.diff(x))))\n\n    def mean_abs_change(x):\n        return np.mean(np.abs(np.diff(x)))\n    \n    for col in df.columns:\n        if col in ['row_id', 'series_id', 'measurement_number']:\n            continue\n        df_out[col + '_mean'] = df.groupby(['series_id'])[col].mean()\n        df_out[col + '_min'] = df.groupby(['series_id'])[col].min()\n        df_out[col + '_max'] = df.groupby(['series_id'])[col].max()\n        df_out[col + '_std'] = df.groupby(['series_id'])[col].std()\n        df_out[col + '_mad'] = df.groupby(['series_id'])[col].mad()\n        df_out[col + '_med'] = df.groupby(['series_id'])[col].median()\n        df_out[col + '_skew'] = df.groupby(['series_id'])[col].skew()\n        df_out[col + '_range'] = df_out[col + '_max'] - df_out[col + '_min']\n        df_out[col + '_max_to_min'] = df_out[col + '_max'] / df_out[col + '_min']\n        df_out[col + '_mean_abs_change'] = df.groupby('series_id')[col].apply(mean_abs_change)\n        df_out[col + '_mean_change_of_abs_change'] = df.groupby('series_id')[col].apply(mean_change_of_abs_change)\n        df_out[col + '_abs_max'] = df.groupby('series_id')[col].apply(lambda x: np.max(np.abs(x)))\n        df_out[col + '_abs_min'] = df.groupby('series_id')[col].apply(lambda x: np.min(np.abs(x)))\n        df_out[col + '_abs_mean'] = df.groupby('series_id')[col].apply(lambda x: np.mean(np.abs(x)))\n        df_out[col + '_abs_std'] = df.groupby('series_id')[col].apply(lambda x: np.std(np.abs(x)))\n        df_out[col + '_abs_avg'] = (df_out[col + '_abs_min'] + df_out[col + '_abs_max'])/2\n        df_out[col + '_abs_range'] = df_out[col + '_abs_max'] - df_out[col + '_abs_min']\n\n    return df_out\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = perform_euler_factors_calculation(train)\ntest = perform_euler_factors_calculation(test)\n\n# checking the shapes of the datset\nprint(\"Shape of train:\", train.shape)\nprint(\"Shape of test:\", test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = train.columns.values[13:23]\nplot_feature_distribution(train, test, 'train', 'test', features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classes = (y_train['surface'].value_counts()).index\ntt = train.merge(y_train, on='series_id', how='inner')\nplot_feature_class_distribution(classes, tt, features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# performing feature engineering on the datasets\n\nx_train = perform_feature_engineering(train)\nx_test = perform_feature_engineering(test)\n\n# checking the shapes\nprint(\"Shape of x_train :\", x_train.shape)\nprint(\"Shape of x_test :\", x_test.shape)\nprint(\"Shape of y_train :\", y_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# applying label encoder to the surface attribute\n\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\ny_train['surface'] = le.fit_transform(y_train['surface'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# replacing all the nan, and infinities with zero\n\nx_train.fillna(0, inplace = True)\nx_train.replace(-np.inf, 0, inplace = True)\nx_train.replace(np.inf, 0, inplace = True)\nx_test.fillna(0, inplace = True)\nx_test.replace(-np.inf, 0, inplace = True)\nx_test.replace(np.inf, 0, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# splitting the data into train and validation sets\n\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size = 0.3,\n                                                      random_state = 0)\n\nprint(\"Shape of x_train :\", x_train.shape)\nprint(\"Shape of x_valid :\", x_valid.shape)\nprint(\"Shape of y_train :\", y_train.shape)\nprint(\"Shape of y_valid :\", y_valid.shape)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# deleting series-id and group-id from y-train and y-valid\n\ny_train = y_train.drop(['series_id', 'group_id'], axis = 1)\ny_valid = y_valid.drop(['series_id', 'group_id'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# modelling\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\n\nmodel_rf = RandomForestClassifier()\nmodel_rf.fit(x_train, y_train)\n\ny_pred_rf = model_rf.predict(x_valid)\n\n# evaluating the model\nprint(\"Training Accuracy :\", model_rf.score(x_train, y_train))\nprint(\"Validation Accuracy :\", model_rf.score(x_valid, y_valid))\n\n# confusion matrix\ncm = confusion_matrix(y_valid, y_pred_rf)\nprint(cm)\n\n# classification report\ncr = classification_report(y_valid, y_pred_rf)\nprint(cr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# modelling\nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\n\nmodel_xgb = XGBClassifier()\nmodel_xgb.fit(x_train, y_train)\n\ny_pred_xgb = model_xgb.predict(x_valid)\n\n# evaluating the model\nprint(\"Training Accuracy :\", model_xgb.score(x_train, y_train))\nprint(\"Validation Accuracy :\", model_xgb.score(x_valid, y_valid))\n\n# confusion matrix\ncm = confusion_matrix(y_valid, y_pred_xgb)\nprint(cm)\n\n# classification report\ncr = classification_report(y_valid, y_pred_xgb)\nprint(cr)","execution_count":37,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n","name":"stderr"},{"output_type":"stream","text":"Training Accuracy : 0.9786276715410573\nValidation Accuracy : 0.8740157480314961\n[[ 46   7   0   0   1   0   0   1   9]\n [  1 201   4   0   2   3   5   2   4]\n [  0   5  75   0   3   9   1   4   9]\n [  0   0   0   3   0   0   0   0   4]\n [  1   4   1   0  89   0   0   0   0]\n [  0   7   2   0   2 201   1   0   3]\n [  0   4   0   0   0   0  85   0   0]\n [  0   4   1   0   0   4   2 144   3]\n [  0  12   3   0   0   9   1   6 155]]\n              precision    recall  f1-score   support\n\n           0       0.96      0.72      0.82        64\n           1       0.82      0.91      0.86       222\n           2       0.87      0.71      0.78       106\n           3       1.00      0.43      0.60         7\n           4       0.92      0.94      0.93        95\n           5       0.89      0.93      0.91       216\n           6       0.89      0.96      0.92        89\n           7       0.92      0.91      0.91       158\n           8       0.83      0.83      0.83       186\n\n   micro avg       0.87      0.87      0.87      1143\n   macro avg       0.90      0.81      0.84      1143\nweighted avg       0.88      0.87      0.87      1143\n\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# boosting the predictions of the model\n\nboosted_predictions = 0.4*y_pred_rf + 0.6*y_pred_xgb\nboosted_predictions","execution_count":42,"outputs":[{"output_type":"execute_result","execution_count":42,"data":{"text/plain":"array([5., 0., 1., ..., 8., 6., 7.])"},"metadata":{}}]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}